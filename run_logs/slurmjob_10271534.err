
CondaError: Run 'conda init' before 'conda activate'

2025-04-28 15:22:07,261	WARNING deprecation.py:50 -- DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
/next/u/stephhk/miniconda3/envs/orpo/lib/python3.9/site-packages/flow/utils/flow_warnings.py:23: PendingDeprecationWarning: The attribute minGap in SumoCarFollowingParams is deprecated, use min_gap instead.
  warnings.warn(
2025-04-28 15:22:08,558	WARNING algorithm_config.py:2604 -- config._enable_rl_module_api was set to False, but no prior exploration config was found to be restored.
2025-04-28 15:22:08,558	WARNING algorithm_config.py:672 -- Cannot create AlgorithmConfig from given `config_dict`! Property sgd_minibatch_size not supported.
INFO - root - Saving experiment results to pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0
WARNING - orpo_experiments - No observers have been added to this run
INFO - orpo_experiments - Running command 'main'
INFO - orpo_experiments - Started
2025-04-28 15:22:11,949	INFO worker.py:1642 -- Started a local Ray instance.
2025-04-28 15:22:12,966	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2025-04-28 15:22:12,967	WARNING algorithm_config.py:2592 -- Setting `exploration_config={'type': 'StochasticSampling'}` because you set `_enable_rl_module_api=False`. This exploration config was restored from a prior exploration config that was overriden when setting `_enable_rl_module_api=True`. This occurs because when RLModule API are enabled, exploration_config can not be set.
[2m[36m(pid=1319726)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:18,522	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:18,525	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:18,525	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:18,537	WARNING catalog.py:629 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:20,703	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:20,703	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:20,703	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:20,703	WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 2025-04-28 15:22:20,706	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319726)[0m 'is_safe_policy' key not found in train batch
[2m[36m(RolloutWorker pid=1319726)[0m 'discriminator_rewards' key not found in train batch
2025-04-28 15:22:20,772	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
2025-04-28 15:22:20,774	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
2025-04-28 15:22:20,775	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2025-04-28 15:22:20,785	WARNING catalog.py:629 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!
2025-04-28 15:22:22,058	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
2025-04-28 15:22:22,058	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2025-04-28 15:22:22,058	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2025-04-28 15:22:22,058	WARNING deprecation.py:50 -- DeprecationWarning: `KLCoeffMixin` has been deprecated. This will raise an error in the future!
/next/u/stephhk/miniconda3/envs/orpo/lib/python3.9/site-packages/torch/nn/modules/linear.py:125: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:145.)
  return F.linear(input, self.weight, self.bias)
2025-04-28 15:22:22,139	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
WARNING - occupancy_measures.agents.orpo - 'is_safe_policy' key not found in train batch
WARNING - occupancy_measures.agents.orpo - 'discriminator_rewards' key not found in train batch
2025-04-28 15:22:22,742	WARNING util.py:68 -- Install gputil for GPU system monitoring.
/next/u/stephhk/miniconda3/envs/orpo/lib/python3.9/site-packages/ray/train/_internal/syncer.py:95: UserWarning: `SyncConfig(upload_dir)` is a deprecated configuration and will be ignored. Please remove it from your `SyncConfig`, as this will raise an error in a future version of Ray.
Please specify `train.RunConfig(storage_path)` instead.
  warnings.warn(
/next/u/stephhk/miniconda3/envs/orpo/lib/python3.9/site-packages/ray/train/_internal/syncer.py:95: UserWarning: `SyncConfig(syncer)` is a deprecated configuration and will be ignored. Please remove it from your `SyncConfig`, as this will raise an error in a future version of Ray.
Please implement custom syncing logic with a custom `pyarrow.fs.FileSystem` instead, and pass it into `train.RunConfig(storage_filesystem)`.
  warnings.warn(
INFO - main - Starting training iteration 0
2025-04-28 15:27:07,501	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.train_one_step` has been deprecated. This will raise an error in the future!
2025-04-28 15:27:07,505	WARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!
/next/u/stephhk/miniconda3/envs/orpo/lib/python3.9/site-packages/torch/autograd/graph.py:824: UserWarning: Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size of 8519680 bytes. (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:145.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
INFO - main - Starting training iteration 1
INFO - main - Starting training iteration 2
INFO - main - Starting training iteration 3
INFO - main - Starting training iteration 4
INFO - main - Starting training iteration 5
INFO - main - Starting training iteration 6
INFO - main - Starting training iteration 7
INFO - main - Starting training iteration 8
INFO - main - Starting training iteration 9
INFO - main - Starting training iteration 10
INFO - main - Starting training iteration 11
INFO - main - Starting training iteration 12
INFO - main - Starting training iteration 13
INFO - main - Starting training iteration 14
INFO - main - Starting training iteration 15
INFO - main - Starting training iteration 16
INFO - main - Starting training iteration 17
INFO - main - Starting training iteration 18
INFO - main - Starting training iteration 19
INFO - main - Starting training iteration 20
INFO - main - Starting training iteration 21
INFO - main - Starting training iteration 22
INFO - main - Starting training iteration 23
INFO - main - Starting training iteration 24
2025-04-28 17:19:17,605	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000025
INFO - main - Starting training iteration 25
INFO - main - Starting training iteration 26
INFO - main - Starting training iteration 27
INFO - main - Starting training iteration 28
INFO - main - Starting training iteration 29
INFO - main - Starting training iteration 30
INFO - main - Starting training iteration 31
INFO - main - Starting training iteration 32
INFO - main - Starting training iteration 33
INFO - main - Starting training iteration 34
INFO - main - Starting training iteration 35
INFO - main - Starting training iteration 36
INFO - main - Starting training iteration 37
INFO - main - Starting training iteration 38
INFO - main - Starting training iteration 39
INFO - main - Starting training iteration 40
INFO - main - Starting training iteration 41
INFO - main - Starting training iteration 42
INFO - main - Starting training iteration 43
INFO - main - Starting training iteration 44
INFO - main - Starting training iteration 45
INFO - main - Starting training iteration 46
INFO - main - Starting training iteration 47
INFO - main - Starting training iteration 48
INFO - main - Starting training iteration 49
2025-04-28 19:15:57,533	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000050
INFO - main - Starting training iteration 50
INFO - main - Starting training iteration 51
INFO - main - Starting training iteration 52
INFO - main - Starting training iteration 53
INFO - main - Starting training iteration 54
INFO - main - Starting training iteration 55
INFO - main - Starting training iteration 56
INFO - main - Starting training iteration 57
INFO - main - Starting training iteration 58
INFO - main - Starting training iteration 59
INFO - main - Starting training iteration 60
INFO - main - Starting training iteration 61
INFO - main - Starting training iteration 62
INFO - main - Starting training iteration 63
INFO - main - Starting training iteration 64
INFO - main - Starting training iteration 65
INFO - main - Starting training iteration 66
INFO - main - Starting training iteration 67
INFO - main - Starting training iteration 68
INFO - main - Starting training iteration 69
INFO - main - Starting training iteration 70
INFO - main - Starting training iteration 71
INFO - main - Starting training iteration 72
INFO - main - Starting training iteration 73
INFO - main - Starting training iteration 74
2025-04-28 21:13:13,339	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000075
INFO - main - Starting training iteration 75
INFO - main - Starting training iteration 76
INFO - main - Starting training iteration 77
INFO - main - Starting training iteration 78
INFO - main - Starting training iteration 79
INFO - main - Starting training iteration 80
INFO - main - Starting training iteration 81
INFO - main - Starting training iteration 82
INFO - main - Starting training iteration 83
INFO - main - Starting training iteration 84
INFO - main - Starting training iteration 85
INFO - main - Starting training iteration 86
INFO - main - Starting training iteration 87
INFO - main - Starting training iteration 88
INFO - main - Starting training iteration 89
INFO - main - Starting training iteration 90
INFO - main - Starting training iteration 91
INFO - main - Starting training iteration 92
INFO - main - Starting training iteration 93
INFO - main - Starting training iteration 94
INFO - main - Starting training iteration 95
INFO - main - Starting training iteration 96
INFO - main - Starting training iteration 97
INFO - main - Starting training iteration 98
INFO - main - Starting training iteration 99
2025-04-28 23:09:53,737	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000100
INFO - main - Starting training iteration 100
INFO - main - Starting training iteration 101
INFO - main - Starting training iteration 102
INFO - main - Starting training iteration 103
INFO - main - Starting training iteration 104
INFO - main - Starting training iteration 105
INFO - main - Starting training iteration 106
INFO - main - Starting training iteration 107
INFO - main - Starting training iteration 108
INFO - main - Starting training iteration 109
INFO - main - Starting training iteration 110
INFO - main - Starting training iteration 111
INFO - main - Starting training iteration 112
INFO - main - Starting training iteration 113
INFO - main - Starting training iteration 114
INFO - main - Starting training iteration 115
INFO - main - Starting training iteration 116
INFO - main - Starting training iteration 117
INFO - main - Starting training iteration 118
INFO - main - Starting training iteration 119
INFO - main - Starting training iteration 120
INFO - main - Starting training iteration 121
INFO - main - Starting training iteration 122
INFO - main - Starting training iteration 123
INFO - main - Starting training iteration 124
2025-04-29 01:06:30,971	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000125
INFO - main - Starting training iteration 125
INFO - main - Starting training iteration 126
INFO - main - Starting training iteration 127
INFO - main - Starting training iteration 128
INFO - main - Starting training iteration 129
INFO - main - Starting training iteration 130
INFO - main - Starting training iteration 131
INFO - main - Starting training iteration 132
INFO - main - Starting training iteration 133
INFO - main - Starting training iteration 134
INFO - main - Starting training iteration 135
INFO - main - Starting training iteration 136
INFO - main - Starting training iteration 137
INFO - main - Starting training iteration 138
INFO - main - Starting training iteration 139
INFO - main - Starting training iteration 140
INFO - main - Starting training iteration 141
INFO - main - Starting training iteration 142
INFO - main - Starting training iteration 143
INFO - main - Starting training iteration 144
INFO - main - Starting training iteration 145
INFO - main - Starting training iteration 146
INFO - main - Starting training iteration 147
INFO - main - Starting training iteration 148
INFO - main - Starting training iteration 149
2025-04-29 03:03:08,825	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000150
INFO - main - Starting training iteration 150
INFO - main - Starting training iteration 151
INFO - main - Starting training iteration 152
INFO - main - Starting training iteration 153
INFO - main - Starting training iteration 154
INFO - main - Starting training iteration 155
INFO - main - Starting training iteration 156
INFO - main - Starting training iteration 157
INFO - main - Starting training iteration 158
INFO - main - Starting training iteration 159
INFO - main - Starting training iteration 160
INFO - main - Starting training iteration 161
INFO - main - Starting training iteration 162
INFO - main - Starting training iteration 163
INFO - main - Starting training iteration 164
INFO - main - Starting training iteration 165
INFO - main - Starting training iteration 166
INFO - main - Starting training iteration 167
INFO - main - Starting training iteration 168
INFO - main - Starting training iteration 169
INFO - main - Starting training iteration 170
INFO - main - Starting training iteration 171
INFO - main - Starting training iteration 172
INFO - main - Starting training iteration 173
INFO - main - Starting training iteration 174
2025-04-29 04:59:46,231	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000175
INFO - main - Starting training iteration 175
INFO - main - Starting training iteration 176
INFO - main - Starting training iteration 177
INFO - main - Starting training iteration 178
INFO - main - Starting training iteration 179
INFO - main - Starting training iteration 180
INFO - main - Starting training iteration 181
INFO - main - Starting training iteration 182
INFO - main - Starting training iteration 183
INFO - main - Starting training iteration 184
INFO - main - Starting training iteration 185
INFO - main - Starting training iteration 186
INFO - main - Starting training iteration 187
INFO - main - Starting training iteration 188
INFO - main - Starting training iteration 189
INFO - main - Starting training iteration 190
INFO - main - Starting training iteration 191
INFO - main - Starting training iteration 192
INFO - main - Starting training iteration 193
INFO - main - Starting training iteration 194
INFO - main - Starting training iteration 195
INFO - main - Starting training iteration 196
INFO - main - Starting training iteration 197
INFO - main - Starting training iteration 198
INFO - main - Starting training iteration 199
2025-04-29 06:56:02,313	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000200
INFO - main - Starting training iteration 200
INFO - main - Starting training iteration 201
INFO - main - Starting training iteration 202
INFO - main - Starting training iteration 203
INFO - main - Starting training iteration 204
INFO - main - Starting training iteration 205
INFO - main - Starting training iteration 206
INFO - main - Starting training iteration 207
INFO - main - Starting training iteration 208
INFO - main - Starting training iteration 209
INFO - main - Starting training iteration 210
INFO - main - Starting training iteration 211
INFO - main - Starting training iteration 212
INFO - main - Starting training iteration 213
INFO - main - Starting training iteration 214
INFO - main - Starting training iteration 215
INFO - main - Starting training iteration 216
INFO - main - Starting training iteration 217
INFO - main - Starting training iteration 218
INFO - main - Starting training iteration 219
INFO - main - Starting training iteration 220
INFO - main - Starting training iteration 221
INFO - main - Starting training iteration 222
INFO - main - Starting training iteration 223
INFO - main - Starting training iteration 224
2025-04-29 08:52:02,889	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000225
INFO - main - Starting training iteration 225
INFO - main - Starting training iteration 226
INFO - main - Starting training iteration 227
INFO - main - Starting training iteration 228
INFO - main - Starting training iteration 229
INFO - main - Starting training iteration 230
INFO - main - Starting training iteration 231
INFO - main - Starting training iteration 232
INFO - main - Starting training iteration 233
INFO - main - Starting training iteration 234
INFO - main - Starting training iteration 235
INFO - main - Starting training iteration 236
INFO - main - Starting training iteration 237
INFO - main - Starting training iteration 238
INFO - main - Starting training iteration 239
INFO - main - Starting training iteration 240
INFO - main - Starting training iteration 241
INFO - main - Starting training iteration 242
INFO - main - Starting training iteration 243
INFO - main - Starting training iteration 244
INFO - main - Starting training iteration 245
INFO - main - Starting training iteration 246
INFO - main - Starting training iteration 247
INFO - main - Starting training iteration 248
INFO - main - Starting training iteration 249
2025-04-29 10:48:38,870	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000250
INFO - main - Starting training iteration 250
INFO - main - Starting training iteration 251
INFO - main - Starting training iteration 252
INFO - main - Starting training iteration 253
INFO - main - Starting training iteration 254
INFO - main - Starting training iteration 255
INFO - main - Starting training iteration 256
INFO - main - Starting training iteration 257
INFO - main - Starting training iteration 258
INFO - main - Starting training iteration 259
2025-04-29 11:35:06,503	WARNING policy.py:137 -- Can not figure out a durable policy name for <class 'occupancy_measures.agents.orpo.ORPOPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.
INFO - main - Saved final checkpoint to data/logs/pandemic/ORPO/true/model_128-128/state-action/weights_10.0_0.1_0.01//seed_0/2025-04-28_15-22-12/checkpoint_000260
INFO - orpo_experiments - Result: {'custom_metrics': {'current/true_reward_mean': -5.188445615951386, 'current/true_reward_min': -7.142462120245878, 'current/true_reward_max': -3.1089087296526094, 'current/proxy_reward_mean': -5.188445615951386, 'current/proxy_reward_min': -7.142462120245878, 'current/proxy_reward_max': -3.1089087296526094, 'current/corr_btw_rewards_mean': 1.0, 'current/corr_btw_rewards_min': 1.0, 'current/corr_btw_rewards_max': 1.0, 'true_reward_mean': -5.188445615951386, 'true_reward_min': -7.142462120245878, 'true_reward_max': -3.1089087296526094, 'proxy_reward_mean': -5.188445615951386, 'proxy_reward_min': -7.142462120245878, 'proxy_reward_max': -3.1089087296526094, 'proxyInfectionSummaryAbsoluteReward_mean': -0.0016839378238341978, 'proxyInfectionSummaryAbsoluteReward_min': -0.0026839378238341987, 'proxyInfectionSummaryAbsoluteReward_max': -0.0008290155440414513, 'proxyLowerStageReward_mean': -0.08048941015292119, 'proxyLowerStageReward_min': -0.09028301141170332, 'proxyLowerStageReward_max': -0.0667828357332954, 'proxySmoothStageChangesReward_mean': -0.19948186528497408, 'proxySmoothStageChangesReward_min': -0.2694300518134715, 'proxySmoothStageChangesReward_max': -0.11398963730569948, 'trueInfectionSummaryAbsoluteReward_mean': -0.0016839378238341978, 'trueInfectionSummaryAbsoluteReward_min': -0.0026839378238341987, 'trueInfectionSummaryAbsoluteReward_max': -0.0008290155440414513, 'truePoliticalReward_mean': 0.0, 'truePoliticalReward_min': 0.0, 'truePoliticalReward_max': 0.0, 'trueLowerStageReward_mean': -0.08048941015292119, 'trueLowerStageReward_min': -0.09028301141170332, 'trueLowerStageReward_max': -0.0667828357332954, 'trueSmoothStageChangesReward_mean': -0.19948186528497408, 'trueSmoothStageChangesReward_min': -0.2694300518134715, 'trueSmoothStageChangesReward_max': -0.11398963730569948}, 'episode_media': {}, 'info': {'learner': {'current': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.119284186913417, 'cur_kl_coeff': 6.402105408309656e-16, 'cur_lr': 0.0003, 'total_loss': -0.030787981989291998, 'policy_loss': -0.01372951865196228, 'vf_loss': 0.06286808957274144, 'vf_explained_var': 0.9073419983570392, 'kl': 0.0017337809627138293, 'entropy': 0.7575846241070674, 'entropy_coeff': 0.06400936, 'discriminator/reward': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 59.38461538461539, 'num_grad_updates_lifetime': 16868.0, 'diff_num_grad_updates_vs_sampler_policy': 16867.0, 'episode_reward_with_discriminator_mean': -5.188445572042838, 'episode_reward_with_unclipped_discriminator_mean': -5.188445572042838}}, 'num_env_steps_sampled': 200720, 'num_env_steps_trained': 401440, 'num_agent_steps_sampled': 200720, 'num_agent_steps_trained': 200720}, 'sampler_results': {'episode_reward_max': -3.1089087296526094, 'episode_reward_min': -7.142462120245878, 'episode_reward_mean': -5.188445615951386, 'episode_len_mean': 193.0, 'episode_media': {}, 'episodes_this_iter': 4, 'policy_reward_min': {'current': -7.142462120245878}, 'policy_reward_max': {'current': -3.1089087296526094}, 'policy_reward_mean': {'current': -5.188445615951386}, 'custom_metrics': {'current/true_reward_mean': -5.188445615951386, 'current/true_reward_min': -7.142462120245878, 'current/true_reward_max': -3.1089087296526094, 'current/proxy_reward_mean': -5.188445615951386, 'current/proxy_reward_min': -7.142462120245878, 'current/proxy_reward_max': -3.1089087296526094, 'current/corr_btw_rewards_mean': 1.0, 'current/corr_btw_rewards_min': 1.0, 'current/corr_btw_rewards_max': 1.0, 'true_reward_mean': -5.188445615951386, 'true_reward_min': -7.142462120245878, 'true_reward_max': -3.1089087296526094, 'proxy_reward_mean': -5.188445615951386, 'proxy_reward_min': -7.142462120245878, 'proxy_reward_max': -3.1089087296526094, 'proxyInfectionSummaryAbsoluteReward_mean': -0.0016839378238341978, 'proxyInfectionSummaryAbsoluteReward_min': -0.0026839378238341987, 'proxyInfectionSummaryAbsoluteReward_max': -0.0008290155440414513, 'proxyLowerStageReward_mean': -0.08048941015292119, 'proxyLowerStageReward_min': -0.09028301141170332, 'proxyLowerStageReward_max': -0.0667828357332954, 'proxySmoothStageChangesReward_mean': -0.19948186528497408, 'proxySmoothStageChangesReward_min': -0.2694300518134715, 'proxySmoothStageChangesReward_max': -0.11398963730569948, 'trueInfectionSummaryAbsoluteReward_mean': -0.0016839378238341978, 'trueInfectionSummaryAbsoluteReward_min': -0.0026839378238341987, 'trueInfectionSummaryAbsoluteReward_max': -0.0008290155440414513, 'truePoliticalReward_mean': 0.0, 'truePoliticalReward_min': 0.0, 'truePoliticalReward_max': 0.0, 'trueLowerStageReward_mean': -0.08048941015292119, 'trueLowerStageReward_min': -0.09028301141170332, 'trueLowerStageReward_max': -0.0667828357332954, 'trueSmoothStageChangesReward_mean': -0.19948186528497408, 'trueSmoothStageChangesReward_min': -0.2694300518134715, 'trueSmoothStageChangesReward_max': -0.11398963730569948}, 'hist_stats': {'current/timestep_true_reward': [0.0, -0.0125, 0.0, 0.0, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.06535533905932737, -0.03, -0.0125, -0.03, -0.0325, -0.03, -0.0325, -0.06535533905932737, -0.0325, -0.06535533905932737, -0.0325, -0.03, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.05, -0.052500000000000005, -0.03, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.02, 0.0, -0.0125, -0.02, -0.02, -0.02, -0.0325, -0.02, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.02, -0.06535533905932737, -0.052500000000000005, -0.052500000000000005, -0.05, -0.04, -0.0325, -0.0325, -0.06535533905932737, -0.06535533905932737, -0.06535533905932737, -0.08535533905932738, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.03, -0.03, -0.052500000000000005, -0.05, -0.05, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.01, -0.06535533905932737, -0.08535533905932738, -0.07535533905932738, -0.07250000000000001, -0.1353553390593274, -0.11, -0.17, -0.17250000000000001, -0.13000000000000003, -0.13250000000000003, -0.13000000000000003, -0.1453553390593274, -0.1125, -0.0925, -0.08535533905932738, -0.08535533905932738, -0.05, -0.05, -0.05, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.052500000000000005, -0.04, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.07250000000000001, -0.07, -0.07250000000000001, -0.05, -0.07250000000000001, -0.1125, -0.13250000000000003, -0.1125, -0.11, -0.0925, -0.0925, -0.08, -0.10535533905932738, -0.07250000000000001, -0.08535533905932738, -0.08535533905932738, -0.07250000000000001, -0.07, -0.08535533905932738, -0.03, -0.0325, -0.03, -0.03, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.0125, -0.0125, -0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.02, -0.02, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.08535533905932738, -0.05, -0.05, -0.07, -0.10535533905932738, -0.07250000000000001, -0.1253553390593274, -0.11, -0.0925, -0.09, -0.1253553390593274, -0.1453553390593274, -0.11, -0.1453553390593274, -0.1453553390593274, -0.0925, -0.09, -0.09, -0.07, -0.07, -0.07250000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.052500000000000005, -0.052500000000000005, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.01, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.06535533905932737, -0.03, -0.07250000000000001, -0.09, -0.1125, -0.13000000000000003, -0.15250000000000002, -0.13250000000000003, -0.1453553390593274, -0.1853553390593274, -0.15250000000000002, -0.1125, -0.11, -0.1125, -0.09, -0.04, -0.08535533905932738, -0.052500000000000005, -0.1253553390593274, -0.1253553390593274, -0.09, -0.07, -0.07, -0.04, -0.08535533905932738, -0.03, -0.02, -0.0325, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.0325, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.0325, -0.0325, -0.0325, -0.0325, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.01, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.05535533905932738, -0.09, -0.09, -0.1253553390593274, -0.10535533905932738, -0.09535533905932739, -0.03, -0.05, -0.08535533905932738, -0.05, -0.07, -0.09, -0.07250000000000001, -0.1253553390593274, -0.1125, -0.1453553390593274, -0.1125, -0.11, -0.13250000000000003, -0.1253553390593274, -0.07250000000000001, -0.07250000000000001, -0.07250000000000001, -0.07, -0.07, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.0325, -0.0325, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.02, -0.02, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.01, -0.045355339059327385, -0.045355339059327385, -0.01, -0.01, -0.01, 0.0, -0.0125, -0.02, -0.0325, -0.08535533905932738, -0.07, -0.10535533905932738, -0.05, -0.02, -0.08535533905932738, -0.05, -0.08535533905932738, -0.08535533905932738, -0.052500000000000005, -0.03, -0.0325, -0.0325, -0.03, -0.0325, -0.02, -0.06535533905932737, -0.03, -0.06535533905932737, -0.0325, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0325, -0.05, -0.0325, -0.01, -0.0125, 0.0, 0.0, -0.045355339059327385, -0.0125, -0.0125, -0.01, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0125, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0], 'current/timestep_proxy_reward': [0.0, -0.0125, 0.0, 0.0, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.06535533905932737, -0.03, -0.0125, -0.03, -0.0325, -0.03, -0.0325, -0.06535533905932737, -0.0325, -0.06535533905932737, -0.0325, -0.03, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.05, -0.052500000000000005, -0.03, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.02, 0.0, -0.0125, -0.02, -0.02, -0.02, -0.0325, -0.02, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.02, -0.06535533905932737, -0.052500000000000005, -0.052500000000000005, -0.05, -0.04, -0.0325, -0.0325, -0.06535533905932737, -0.06535533905932737, -0.06535533905932737, -0.08535533905932738, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.03, -0.03, -0.052500000000000005, -0.05, -0.05, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.01, -0.06535533905932737, -0.08535533905932738, -0.07535533905932738, -0.07250000000000001, -0.1353553390593274, -0.11, -0.17, -0.17250000000000001, -0.13000000000000003, -0.13250000000000003, -0.13000000000000003, -0.1453553390593274, -0.1125, -0.0925, -0.08535533905932738, -0.08535533905932738, -0.05, -0.05, -0.05, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.052500000000000005, -0.04, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.07250000000000001, -0.07, -0.07250000000000001, -0.05, -0.07250000000000001, -0.1125, -0.13250000000000003, -0.1125, -0.11, -0.0925, -0.0925, -0.08, -0.10535533905932738, -0.07250000000000001, -0.08535533905932738, -0.08535533905932738, -0.07250000000000001, -0.07, -0.08535533905932738, -0.03, -0.0325, -0.03, -0.03, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.0125, -0.0125, -0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.02, -0.02, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.08535533905932738, -0.05, -0.05, -0.07, -0.10535533905932738, -0.07250000000000001, -0.1253553390593274, -0.11, -0.0925, -0.09, -0.1253553390593274, -0.1453553390593274, -0.11, -0.1453553390593274, -0.1453553390593274, -0.0925, -0.09, -0.09, -0.07, -0.07, -0.07250000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.052500000000000005, -0.052500000000000005, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.01, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.06535533905932737, -0.03, -0.07250000000000001, -0.09, -0.1125, -0.13000000000000003, -0.15250000000000002, -0.13250000000000003, -0.1453553390593274, -0.1853553390593274, -0.15250000000000002, -0.1125, -0.11, -0.1125, -0.09, -0.04, -0.08535533905932738, -0.052500000000000005, -0.1253553390593274, -0.1253553390593274, -0.09, -0.07, -0.07, -0.04, -0.08535533905932738, -0.03, -0.02, -0.0325, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.0325, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.0325, -0.0325, -0.0325, -0.0325, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.01, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.05535533905932738, -0.09, -0.09, -0.1253553390593274, -0.10535533905932738, -0.09535533905932739, -0.03, -0.05, -0.08535533905932738, -0.05, -0.07, -0.09, -0.07250000000000001, -0.1253553390593274, -0.1125, -0.1453553390593274, -0.1125, -0.11, -0.13250000000000003, -0.1253553390593274, -0.07250000000000001, -0.07250000000000001, -0.07250000000000001, -0.07, -0.07, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.0325, -0.0325, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.02, -0.02, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.01, -0.045355339059327385, -0.045355339059327385, -0.01, -0.01, -0.01, 0.0, -0.0125, -0.02, -0.0325, -0.08535533905932738, -0.07, -0.10535533905932738, -0.05, -0.02, -0.08535533905932738, -0.05, -0.08535533905932738, -0.08535533905932738, -0.052500000000000005, -0.03, -0.0325, -0.0325, -0.03, -0.0325, -0.02, -0.06535533905932737, -0.03, -0.06535533905932737, -0.0325, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0325, -0.05, -0.0325, -0.01, -0.0125, 0.0, 0.0, -0.045355339059327385, -0.0125, -0.0125, -0.01, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0125, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0], 'episode_reward': [-3.1089087296526094, -5.482462120245885, -7.142462120245878, -5.01994949366117], 'episode_lengths': [193, 193, 193, 193], 'policy_current_reward': [-3.1089087296526094, -5.482462120245885, -7.142462120245878, -5.01994949366117]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.5269049220290825, 'mean_inference_ms': 1.4681105378000487, 'mean_action_processing_ms': 0.10342980244561154, 'mean_env_wait_ms': 720.1398319438862, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {}}, 'episode_reward_max': -3.1089087296526094, 'episode_reward_min': -7.142462120245878, 'episode_reward_mean': -5.188445615951386, 'episode_len_mean': 193.0, 'episodes_this_iter': 4, 'policy_reward_min': {'current': -7.142462120245878}, 'policy_reward_max': {'current': -3.1089087296526094}, 'policy_reward_mean': {'current': -5.188445615951386}, 'hist_stats': {'current/timestep_true_reward': [0.0, -0.0125, 0.0, 0.0, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.06535533905932737, -0.03, -0.0125, -0.03, -0.0325, -0.03, -0.0325, -0.06535533905932737, -0.0325, -0.06535533905932737, -0.0325, -0.03, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.05, -0.052500000000000005, -0.03, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.02, 0.0, -0.0125, -0.02, -0.02, -0.02, -0.0325, -0.02, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.02, -0.06535533905932737, -0.052500000000000005, -0.052500000000000005, -0.05, -0.04, -0.0325, -0.0325, -0.06535533905932737, -0.06535533905932737, -0.06535533905932737, -0.08535533905932738, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.03, -0.03, -0.052500000000000005, -0.05, -0.05, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.01, -0.06535533905932737, -0.08535533905932738, -0.07535533905932738, -0.07250000000000001, -0.1353553390593274, -0.11, -0.17, -0.17250000000000001, -0.13000000000000003, -0.13250000000000003, -0.13000000000000003, -0.1453553390593274, -0.1125, -0.0925, -0.08535533905932738, -0.08535533905932738, -0.05, -0.05, -0.05, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.052500000000000005, -0.04, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.07250000000000001, -0.07, -0.07250000000000001, -0.05, -0.07250000000000001, -0.1125, -0.13250000000000003, -0.1125, -0.11, -0.0925, -0.0925, -0.08, -0.10535533905932738, -0.07250000000000001, -0.08535533905932738, -0.08535533905932738, -0.07250000000000001, -0.07, -0.08535533905932738, -0.03, -0.0325, -0.03, -0.03, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.0125, -0.0125, -0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.02, -0.02, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.08535533905932738, -0.05, -0.05, -0.07, -0.10535533905932738, -0.07250000000000001, -0.1253553390593274, -0.11, -0.0925, -0.09, -0.1253553390593274, -0.1453553390593274, -0.11, -0.1453553390593274, -0.1453553390593274, -0.0925, -0.09, -0.09, -0.07, -0.07, -0.07250000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.052500000000000005, -0.052500000000000005, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.01, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.06535533905932737, -0.03, -0.07250000000000001, -0.09, -0.1125, -0.13000000000000003, -0.15250000000000002, -0.13250000000000003, -0.1453553390593274, -0.1853553390593274, -0.15250000000000002, -0.1125, -0.11, -0.1125, -0.09, -0.04, -0.08535533905932738, -0.052500000000000005, -0.1253553390593274, -0.1253553390593274, -0.09, -0.07, -0.07, -0.04, -0.08535533905932738, -0.03, -0.02, -0.0325, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.0325, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.0325, -0.0325, -0.0325, -0.0325, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.01, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.05535533905932738, -0.09, -0.09, -0.1253553390593274, -0.10535533905932738, -0.09535533905932739, -0.03, -0.05, -0.08535533905932738, -0.05, -0.07, -0.09, -0.07250000000000001, -0.1253553390593274, -0.1125, -0.1453553390593274, -0.1125, -0.11, -0.13250000000000003, -0.1253553390593274, -0.07250000000000001, -0.07250000000000001, -0.07250000000000001, -0.07, -0.07, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.0325, -0.0325, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.02, -0.02, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.01, -0.045355339059327385, -0.045355339059327385, -0.01, -0.01, -0.01, 0.0, -0.0125, -0.02, -0.0325, -0.08535533905932738, -0.07, -0.10535533905932738, -0.05, -0.02, -0.08535533905932738, -0.05, -0.08535533905932738, -0.08535533905932738, -0.052500000000000005, -0.03, -0.0325, -0.0325, -0.03, -0.0325, -0.02, -0.06535533905932737, -0.03, -0.06535533905932737, -0.0325, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0325, -0.05, -0.0325, -0.01, -0.0125, 0.0, 0.0, -0.045355339059327385, -0.0125, -0.0125, -0.01, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0125, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0], 'current/timestep_proxy_reward': [0.0, -0.0125, 0.0, 0.0, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.06535533905932737, -0.03, -0.0125, -0.03, -0.0325, -0.03, -0.0325, -0.06535533905932737, -0.0325, -0.06535533905932737, -0.0325, -0.03, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.05, -0.052500000000000005, -0.03, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.02, 0.0, -0.0125, -0.02, -0.02, -0.02, -0.0325, -0.02, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.02, -0.06535533905932737, -0.052500000000000005, -0.052500000000000005, -0.05, -0.04, -0.0325, -0.0325, -0.06535533905932737, -0.06535533905932737, -0.06535533905932737, -0.08535533905932738, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.052500000000000005, -0.03, -0.03, -0.052500000000000005, -0.05, -0.05, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.01, -0.06535533905932737, -0.08535533905932738, -0.07535533905932738, -0.07250000000000001, -0.1353553390593274, -0.11, -0.17, -0.17250000000000001, -0.13000000000000003, -0.13250000000000003, -0.13000000000000003, -0.1453553390593274, -0.1125, -0.0925, -0.08535533905932738, -0.08535533905932738, -0.05, -0.05, -0.05, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.052500000000000005, -0.04, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.045355339059327385, -0.045355339059327385, -0.06535533905932737, -0.07250000000000001, -0.07, -0.07250000000000001, -0.05, -0.07250000000000001, -0.1125, -0.13250000000000003, -0.1125, -0.11, -0.0925, -0.0925, -0.08, -0.10535533905932738, -0.07250000000000001, -0.08535533905932738, -0.08535533905932738, -0.07250000000000001, -0.07, -0.08535533905932738, -0.03, -0.0325, -0.03, -0.03, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.0125, -0.0125, -0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, -0.045355339059327385, -0.01, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.02, -0.02, -0.02, -0.02, -0.02, -0.0325, -0.02, -0.0325, -0.02, -0.02, -0.0325, -0.0325, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.045355339059327385, -0.08535533905932738, -0.05, -0.05, -0.07, -0.10535533905932738, -0.07250000000000001, -0.1253553390593274, -0.11, -0.0925, -0.09, -0.1253553390593274, -0.1453553390593274, -0.11, -0.1453553390593274, -0.1453553390593274, -0.0925, -0.09, -0.09, -0.07, -0.07, -0.07250000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.06000000000000001, -0.052500000000000005, -0.052500000000000005, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, -0.0125, -0.0125, -0.0125, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, 0.0, -0.0125, -0.045355339059327385, -0.01, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.06535533905932737, -0.03, -0.07250000000000001, -0.09, -0.1125, -0.13000000000000003, -0.15250000000000002, -0.13250000000000003, -0.1453553390593274, -0.1853553390593274, -0.15250000000000002, -0.1125, -0.11, -0.1125, -0.09, -0.04, -0.08535533905932738, -0.052500000000000005, -0.1253553390593274, -0.1253553390593274, -0.09, -0.07, -0.07, -0.04, -0.08535533905932738, -0.03, -0.02, -0.0325, -0.0325, -0.0325, -0.02, -0.02, -0.02, -0.0325, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.052500000000000005, -0.052500000000000005, -0.04, -0.04, -0.04, -0.04, -0.04, -0.052500000000000005, -0.0325, -0.0325, -0.0325, -0.0325, -0.0125, 0.0, 0.0, 0.0, -0.045355339059327385, -0.01, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.05535533905932738, -0.09, -0.09, -0.1253553390593274, -0.10535533905932738, -0.09535533905932739, -0.03, -0.05, -0.08535533905932738, -0.05, -0.07, -0.09, -0.07250000000000001, -0.1253553390593274, -0.1125, -0.1453553390593274, -0.1125, -0.11, -0.13250000000000003, -0.1253553390593274, -0.07250000000000001, -0.07250000000000001, -0.07250000000000001, -0.07, -0.07, -0.04, -0.04, -0.04, -0.04, -0.04, -0.04, -0.0325, -0.0325, -0.0325, -0.02, -0.0325, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, -0.0125, 0.0, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0125, 0.0, -0.0125, -0.02, -0.02, -0.0125, -0.0125, -0.0125, -0.045355339059327385, -0.06535533905932737, -0.01, -0.045355339059327385, -0.045355339059327385, -0.01, -0.01, -0.01, 0.0, -0.0125, -0.02, -0.0325, -0.08535533905932738, -0.07, -0.10535533905932738, -0.05, -0.02, -0.08535533905932738, -0.05, -0.08535533905932738, -0.08535533905932738, -0.052500000000000005, -0.03, -0.0325, -0.0325, -0.03, -0.0325, -0.02, -0.06535533905932737, -0.03, -0.06535533905932737, -0.0325, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0325, -0.05, -0.0325, -0.01, -0.0125, 0.0, 0.0, -0.045355339059327385, -0.0125, -0.0125, -0.01, -0.0125, 0.0, 0.0, -0.0125, 0.0, -0.0125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.045355339059327385, -0.045355339059327385, -0.01, -0.0125, -0.01, 0.0, -0.0125, -0.0125, -0.0125, 0.0, 0.0], 'episode_reward': [-3.1089087296526094, -5.482462120245885, -7.142462120245878, -5.01994949366117], 'episode_lengths': [193, 193, 193, 193], 'policy_current_reward': [-3.1089087296526094, -5.482462120245885, -7.142462120245878, -5.01994949366117]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.5269049220290825, 'mean_inference_ms': 1.4681105378000487, 'mean_action_processing_ms': 0.10342980244561154, 'mean_env_wait_ms': 720.1398319438862, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 200720, 'num_agent_steps_trained': 200720, 'num_env_steps_sampled': 200720, 'num_env_steps_trained': 401440, 'num_env_steps_sampled_this_iter': 772, 'num_env_steps_trained_this_iter': 1544, 'num_env_steps_sampled_throughput_per_sec': 2.766405810022872, 'num_env_steps_trained_throughput_per_sec': 5.532811620045744, 'timesteps_total': 200720, 'num_steps_trained_this_iter': 1544, 'agent_timesteps_total': 200720, 'timers': {'training_iteration_time_ms': 278717.752, 'learn_time_ms': 289.722, 'learn_throughput': 2664.619, 'synch_weights_time_ms': 3.511}, 'counters': {'num_env_steps_sampled': 200720, 'num_env_steps_trained': 401440, 'num_agent_steps_sampled': 200720, 'num_agent_steps_trained': 200720}, 'done': False, 'episodes_total': 1040, 'training_iteration': 260, 'trial_id': 'default', 'date': '2025-04-29_11-35-06', 'timestamp': 1745951706, 'time_this_iter_s': 279.0711054801941, 'time_total_s': 72754.07941508293, 'pid': 1319235, 'hostname': 'next5.stanford.edu', 'node_ip': '10.79.12.129', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'pandemic_env_multiagent', 'env_config': {'sim_config': PandemicSimConfig(num_persons=500, delta_start_lo=95, delta_start_hi=105, location_configs=[LocationConfig(location_type=<class 'pandemic_simulator.environment.location.home.Home'>, num=150, num_assignees=-1, state_opts={}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.store.GroceryStore'>, num=2, num_assignees=5, state_opts={'visitor_capacity': 30}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.office.Office'>, num=2, num_assignees=150, state_opts={'visitor_capacity': 0}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.school.School'>, num=10, num_assignees=2, state_opts={'visitor_capacity': 30}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.hospital.Hospital'>, num=1, num_assignees=15, state_opts={'patient_capacity': 5}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.store.RetailStore'>, num=2, num_assignees=5, state_opts={'visitor_capacity': 30}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>, num=2, num_assignees=3, state_opts={'visitor_capacity': 5}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.restaurant.Restaurant'>, num=1, num_assignees=6, state_opts={'visitor_capacity': 30}, extra_opts={}), LocationConfig(location_type=<class 'pandemic_simulator.environment.location.restaurant.Bar'>, num=1, num_assignees=3, state_opts={'visitor_capacity': 30}, extra_opts={})], regulation_compliance_prob=0.99, max_hospital_capacity=5, person_routine_assignment=<pandemic_simulator.script_helpers.person_routines.DefaultPersonRoutineAssignment object at 0x152a57e1bfa0>), 'sim_opts': PandemicSimOpts(infection_spread_rate_mean=0.021, infection_spread_rate_sigma=0.01, infection_delta_spread_rate_mean=0.03, infection_delta_spread_rate_sigma=0.01, spontaneous_testing_rate=0.3, symp_testing_rate=0.3, critical_testing_rate=1.0, testing_false_positive_rate=0.001, testing_false_negative_rate=0.01, retest_rate=0.033, sim_steps_per_regulation=24, use_contact_tracer=False, contact_tracer_history_size=5, infection_threshold=10), 'pandemic_regulations': [PandemicRegulation(location_type_to_rule_kwargs={<class 'pandemic_simulator.environment.location.office.Office'>: {'lock': False}, <class 'pandemic_simulator.environment.location.school.School'>: {'lock': False}, <class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>: {'lock': False}, <class 'pandemic_simulator.environment.location.store.RetailStore'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Bar'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Restaurant'>: {'lock': False}}, business_type_to_rule_kwargs=None, social_distancing=0, quarantine=False, quarantine_if_contact_positive=False, quarantine_if_household_quarantined=False, stay_home_if_sick=False, practice_good_hygiene=False, wear_facial_coverings=False, risk_to_avoid_gathering_size={<Risk.LOW: 0>: -1, <Risk.HIGH: 1>: -1}, risk_to_avoid_location_types=None, stage=0), PandemicRegulation(location_type_to_rule_kwargs={<class 'pandemic_simulator.environment.location.office.Office'>: {'lock': False}, <class 'pandemic_simulator.environment.location.school.School'>: {'lock': False}, <class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>: {'lock': False}, <class 'pandemic_simulator.environment.location.store.RetailStore'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Restaurant'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Bar'>: {'lock': False}}, business_type_to_rule_kwargs=None, social_distancing=0, quarantine=False, quarantine_if_contact_positive=False, quarantine_if_household_quarantined=False, stay_home_if_sick=True, practice_good_hygiene=True, wear_facial_coverings=False, risk_to_avoid_gathering_size={<Risk.HIGH: 1>: 25, <Risk.LOW: 0>: 50}, risk_to_avoid_location_types=None, stage=1), PandemicRegulation(location_type_to_rule_kwargs={<class 'pandemic_simulator.environment.location.office.Office'>: {'lock': False}, <class 'pandemic_simulator.environment.location.school.School'>: {'lock': True}, <class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>: {'lock': True}, <class 'pandemic_simulator.environment.location.store.RetailStore'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Restaurant'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Bar'>: {'lock': False}}, business_type_to_rule_kwargs=None, social_distancing=0.3, quarantine=False, quarantine_if_contact_positive=False, quarantine_if_household_quarantined=False, stay_home_if_sick=True, practice_good_hygiene=True, wear_facial_coverings=True, risk_to_avoid_gathering_size={<Risk.HIGH: 1>: 10, <Risk.LOW: 0>: 25}, risk_to_avoid_location_types=None, stage=2), PandemicRegulation(location_type_to_rule_kwargs={<class 'pandemic_simulator.environment.location.office.Office'>: {'lock': False}, <class 'pandemic_simulator.environment.location.school.School'>: {'lock': True}, <class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>: {'lock': True}, <class 'pandemic_simulator.environment.location.store.RetailStore'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Restaurant'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Bar'>: {'lock': False}}, business_type_to_rule_kwargs=None, social_distancing=0.5, quarantine=False, quarantine_if_contact_positive=False, quarantine_if_household_quarantined=False, stay_home_if_sick=True, practice_good_hygiene=True, wear_facial_coverings=True, risk_to_avoid_gathering_size={<Risk.HIGH: 1>: 0, <Risk.LOW: 0>: 0}, risk_to_avoid_location_types=None, stage=3), PandemicRegulation(location_type_to_rule_kwargs={<class 'pandemic_simulator.environment.location.office.Office'>: {'lock': True}, <class 'pandemic_simulator.environment.location.school.School'>: {'lock': True}, <class 'pandemic_simulator.environment.location.hair_salon.HairSalon'>: {'lock': True}, <class 'pandemic_simulator.environment.location.store.RetailStore'>: {'lock': True}, <class 'pandemic_simulator.environment.location.restaurant.Restaurant'>: {'lock': False}, <class 'pandemic_simulator.environment.location.restaurant.Bar'>: {'lock': False}}, business_type_to_rule_kwargs=None, social_distancing=0.7, quarantine=False, quarantine_if_contact_positive=False, quarantine_if_household_quarantined=False, stay_home_if_sick=True, practice_good_hygiene=True, wear_facial_coverings=True, risk_to_avoid_gathering_size={<Risk.HIGH: 1>: 0, <Risk.LOW: 0>: 0}, risk_to_avoid_location_types=None, stage=4)], 'done_fn': <pandemic_simulator.environment.done.TimeLimitDone object at 0x152a59d299a0>, 'reward_fun': 'true', 'true_reward_fun': <pandemic_simulator.environment.reward.SumReward object at 0x150412d73730>, 'proxy_reward_fun': <pandemic_simulator.environment.reward.SumReward object at 0x150412d739a0>, 'constrain': True, 'four_start': False, 'obs_history_size': 3, 'num_days_in_obs': 8, 'use_safe_policy_actions': False, 'safe_policy': 'swedish_strategy', 'horizon': 192}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': True, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': False, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 386, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0003, 'grad_clip': 10, 'grad_clip_by': 'global_norm', 'train_batch_size': 772, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [128, 128], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'model_with_discriminator', 'custom_model_config': {'discriminator_depth': 2, 'discriminator_width': 256, 'discriminator_state_dim': 13, 'use_action_for_disc': True, 'use_history_for_disc': False, 'time_dim': 1, 'history_range': (-1, 0)}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function create_multiagent.<locals>.policy_mapping_fn at 0x152a54d6b700>, 'policies_to_train': ['current'], 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 1, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': 0, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': None, '_enable_rl_module_api': False, '_AlgorithmConfig__prior_exploration_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 64, 'num_sgd_iter': 5, 'shuffle_sequences': True, 'vf_loss_coeff': 0.5, 'entropy_coeff': 0.01, 'entropy_coeff_schedule': [[0, 0.1], [500000, 0.01]], 'clip_param': 0.3, 'vf_clip_param': 20, 'vf_share_layers': -1, 'om_divergence_coeffs': {}, 'om_divergence_type': {}, 'action_dist_divergence_coeff': None, 'action_dist_divergence_type': 'kl', 'update_safe_policy_freq': None, 'safe_policy_ids': [], 'current_policy_id': 'current', 'train_discriminator_first': True, 'discriminator_reward_clip': 100, 'num_extra_repeated_safe_policy_batches': 1, 'discriminator_state_info_key': None, 'discriminator_num_sgd_iter': 2, 'wgan_grad_clip': 0.01, 'wgan_grad_penalty_weight': None, 'wasserstein_distance_subtract_mean_safe_policy_score': False, 'split_om_kl': False, 'occupancy_measure_kl_target': {}, 'use_squared_kl_adaptive_coefficient': False, 'lambda': 0.95, 'input': 'sampler', 'policies': {'current': (None, None, None, None)}, 'callbacks': <class 'occupancy_measures.envs.pandemic_callbacks.PandemicCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 72754.07941508293, 'iterations_since_restore': 260, 'perf': {'cpu_util_percent': 6.275879396984924, 'ram_util_percent': 2.6999999999999997}}
INFO - orpo_experiments - Completed after 20:12:58
[2m[36m(pid=1319727)[0m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=1319727)[0m 2025-04-28 15:22:18,556	WARNING algorithm_config.py:2578 -- Setting `exploration_config={}` because you set `_enable_rl_module_api=True`. When RLModule API are enabled, exploration_config can not be set. If you want to implement custom exploration behaviour, please modify the `forward_exploration` method of the RLModule at hand. On configs that have a default exploration config, this must be done with `config.exploration_config={}`.
[2m[36m(RolloutWorker pid=1319727)[0m 2025-04-28 15:22:18,570	WARNING catalog.py:629 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!
[2m[36m(RolloutWorker pid=1319727)[0m 'is_safe_policy' key not found in train batch
[2m[36m(RolloutWorker pid=1319727)[0m 'discriminator_rewards' key not found in train batch
